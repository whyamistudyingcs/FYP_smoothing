{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8117103,"sourceType":"datasetVersion","datasetId":4795846}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/whyamistudyingcs/FYP_smoothing.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-14T14:19:47.437916Z","iopub.execute_input":"2024-04-14T14:19:47.438265Z","iopub.status.idle":"2024-04-14T14:19:48.932424Z","shell.execute_reply.started":"2024-04-14T14:19:47.438233Z","shell.execute_reply":"2024-04-14T14:19:48.931347Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'FYP_smoothing'...\nremote: Enumerating objects: 35, done.\u001b[K\nremote: Counting objects: 100% (35/35), done.\u001b[K\nremote: Compressing objects: 100% (32/32), done.\u001b[K\nremote: Total 35 (delta 11), reused 12 (delta 3), pack-reused 0\u001b[K\nUnpacking objects: 100% (35/35), 12.41 KiB | 1.24 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nargs = {\n    'exp_dir': \"./experiments/cls/\",\n    'exp_msg': \"CLS Transformer\",\n    'gpu_idx': 10,\n\n    'eval': True, #\n    'model_dir_path': '/kaggle/input/imdb-bert-5', #\n    'save_model': 'cls_trans', #\n    'load_model': 'cls_trans_4', #\n    'save': True, #\n\n    'seed': 42, # \n    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\", #\n\n    'w_gn': 1.0,                    # embedding noise inference\n    'noise_eps': 0.2,               # noise inference\n    'single_layer': False,  #\n    'nth_layers': 2,                # interval of injecting noise\n\n    'num_ensemble': 10, # k_0\n    'binom_ensemble': 50, # k_1\n    'pooler_output': False, #\n    'custom_forward': True,         # noise forwarding\n    'binom_n_eval': 5,\n\n    'sample_mask': False,\n    'mask_batch_ratio': 1.0,        # percentage of masking , max is multi mask\n    'rand_mask': False,\n    'grad_mask_sample': False,\n    'ens_grad_mask': 'rand',\n    'two_step': False,\n    'multi_mask': 2,                # number of masks\n    'mask_idx': 103, #\n    'vote_type': 'avg',\n\n    'epochs': 5,\n    'batch_size': 16, #\n    'model': 'bert',                # model\n    'optim': 'adamw',\n    'scheduler': 'linear',\n    'lr': 0.00001,\n    'dropout': 0.1,\n    'clip': 1.0,                    # clip gradient \n    'margin': 0.5,\n\n    'embed_dim': 768,\n\n    'dataset': 'imdb',#\n    'num_classes': 2, #\n    'pad_idx': 0,\n    'max_seq_length': 256, #\n\n    'binom_p': False, #\n    'alpha_p': 0.70 #\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:19:48.936086Z","iopub.execute_input":"2024-04-14T14:19:48.936398Z","iopub.status.idle":"2024-04-14T14:19:52.590804Z","shell.execute_reply.started":"2024-04-14T14:19:48.936369Z","shell.execute_reply":"2024-04-14T14:19:52.589816Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/FYP_smoothing')\n\nimport torch\nfrom transformers import AdamW\nimport sys\nimport numpy as np\nimport random\nimport time, datetime\nfrom datetime import timedelta\nfrom model.adv_model import *\nfrom model.load_model import *\n\nfrom utils.utils import *\nfrom utils.dataloader import *\n\nif args['seed']>0:\n    SEED = args['seed']\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n\ntokenizer = load_tokenizer(args)\ntrain_dataloader, test_dataloader, dev_dataloader = trans_dataloader(args['dataset'], tokenizer, args)\n\nprint(f\"Dataset classes: {args['num_classes']}\") \ntrain_niter = len(train_dataloader)\ntotal_iter = len(train_dataloader) * args['epochs']\n# Create Model \nprint(f\"Load Model...\")\nmodel = noisy_forward_loader(args)\nmodel = SeqClsWrapper(model, args)\n\nif args['eval'] == True:\n    model = load_checkpoint(model, args['load_model'], args['model_dir_path'])\n    model.to(args['device'])\n    model.eval()\n    \n    TP = 0\n    n_samples = len(test_dataloader.dataset)\n    \n    start_t_gen = time.perf_counter()\n    print(\"Start Evaluation....\")\n    for batch_idx, batch in enumerate(test_dataloader):\n        \n        input_ids = batch['input_ids'].to(args['device'])\n        attention_mask = batch['attention_mask'].to(args['device'])\n        labels = batch['labels'].to(args['device'])\n        \n        if args['num_ensemble'] > 1:\n            mask_indices, _ = model.grad_mask(input_ids, attention_mask)\n            logits = model.two_step_ensemble(input_ids, attention_mask, mask_indices, args['num_ensemble'], args['binom_ensemble'])\n            correct = logits.argmax(dim=-1).eq(labels)\n            TP += correct.sum().item()\n        else:\n            if args['multi_mask'] > 0:\n                mask_indices, _ = model.grad_mask(input_ids, attention_mask)\n                masked_ids = input_masking_function(input_ids, mask_indices, args)\n                with torch.no_grad():\n                    output = model(masked_ids, attention_mask)\n            \n            else:\n                with torch.no_grad():\n                    output = model(input_ids, attention_mask)\n            \n            preds = output['logits']\n            correct = preds.argmax(dim=-1).eq(labels)\n            TP += correct.sum().item()\n    \n    acc = 100 * (TP / n_samples)\n    \n    eval_t = time.perf_counter() - start_t_gen\n    log = f\"Test Acc: {acc:.4f}\"\n    print(log)\n    print(f\"Total Evaluation Time: {timedelta(seconds=eval_t)}\") \n    sys.exit(0)\n    \nelse:\n    model.to(args['device'])\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=args['lr'])\n\nprint(\"Start Training...\")\nstart_train = time.perf_counter()\n\n# TODO: print args\n\nbest_dev_epoch = 0\nbest_dev_acc = 0\nfor epoch in range(args['epochs']):\n    model.train()\n    loss_epoch = []\n    loss_ood_epoch = []\n    \n    for batch_idx, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids'].to(args['device'])\n        attention_mask = batch['attention_mask'].to(args['device'])\n        labels = batch['labels'].to(args['device'])\n        \n        model.eval()\n        indices, delta_grad = model.grad_mask(input_ids, attention_mask, pred=labels, mask_filter=True)\n        model.zero_grad() # ensure no gradient left after grad mask call\n        # print(delta_grad.shape)\n        \n        masked_ids = input_masking_function(input_ids, indices, args) # [batch, seq_length]\n        \n        model.train()\n        output = model(masked_ids, attention_mask, labels, delta_grad, indices)\n        \n        loss = output['loss']\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n        optimizer.step()\n        loss_epoch.append(loss.item())\n        \n        if batch_idx % 100 == 0:\n            log = f\"Epoch: {epoch} || Iter: {batch_idx} || Loss: {np.mean(loss_epoch[-100:]):.3f}\"\n            print(log)\n        \n        # scheduler step\n        curr = epoch * train_niter + batch_idx\n        LinearScheduler(optimizer, total_iter, curr, args['lr'])\n        \n    log = f\"\\nEpoch: {epoch} || Loss: {np.mean(loss_epoch):.3f}\"\n    print(log)\n    \n    dev_acc = model_evaluation(model, dev_dataloader,args, eval_mode='dev')\n    \n    if dev_acc > best_dev_acc:\n        best_dev_acc = dev_acc\n        best_dev_epoch = epoch\n        if args[\"save\"]:\n            save_checkpoint(args[\"save_model\"], model, epoch, ckpt_dir=args['model_dir_path'])\n    \n    log = f\"Epoch: {epoch} || Dev Acc: {dev_acc:.4f} || BestDevAcc: {best_dev_acc:.4f} || BestEpoch: {best_dev_epoch}\"\n    print(log)\n\nend_train = time.perf_counter() - start_train\nlog = f\"Total Training Time: {timedelta(seconds=end_train)}\"\nprint(log)\n\nprint(\"Start testSet Evaluation...\")\nload_model_name = args['save_model'] + f\"_{best_dev_epoch}\"\nprint(f\"Load BestDev Model...: {load_model_name}\") \n\nmodel = noisy_forward_loader(args)\nmodel = SeqClsWrapper(model, args)\nmodel = load_checkpoint(model, load_model_name, args['model_dir_path'])\nmodel.to(args['device'])\nmodel.eval()\n\ntest_acc = model_evaluation(model, test_dataloader, args, eval_mode='test')\nlog = f\"TestAcc: {test_acc:.4f} || BestDevAcc: {best_dev_acc:.4f}\"\nprint(log) \nprint(\"End Training...\")","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:19:52.592633Z","iopub.execute_input":"2024-04-14T14:19:52.593016Z","iopub.status.idle":"2024-04-14T14:21:50.545567Z","shell.execute_reply.started":"2024-04-14T14:19:52.592991Z","shell.execute_reply":"2024-04-14T14:21:50.544335Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Load Tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce646c572a5c433ba7c5ed323ecd5986"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a453410bccc46949a808ad8a12d74f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b0f3492e3e46e9bf103e7dcd2415d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"557863c40a2a4b20b311fe3d39340773"}},"metadata":{}},{"name":"stdout","text":"Tokenizer: bert || PAD: 0 || MASK: 103\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"454a3388f6f84815859930bea7ec17ed"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 38.0MB/s]\nDownloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 76.9MB/s]\nDownloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 127MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8235afbd685d4679802e476a7dc196b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cab6a1a30ce44f2c9eae96fb3faeceac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e6803402c243fdbc9e6e20505ab152"}},"metadata":{}},{"name":"stdout","text":"Trainset Size: 1125\nTestset Size: 750\nDevset Size: 125\nDataset classes: 2\nLoad Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69388ef9936c4b1699673dd4798dfa83"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Load: cls_trans_4\nStart Evaluation....\nTest Acc: 92.0000\nTotal Evaluation Time: 0:01:15.892583\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"],"ename":"SystemExit","evalue":"0","output_type":"error"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n","output_type":"stream"}]}]}